## 20190802

[TOC]

### Running frist app on kubernetes

简单起见，可以使用kubectl run 命令创建所有必须的组件

**kubectl run kubia --image=kubia --port=8899 --generator=run/v1**

```
lucheng@ubuntu_18_04:~/test$ sudo kubectl run kubia --image=kubia --port=8899 --generator=run/v1
kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
replicationcontroller/kubia created

lucheng@ubuntu_18_04:~$ sudo kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY   STATUS              RESTARTS   AGE
kube-system   kube-addon-manager-minikube   0/1     ContainerCreating   0          28m

lucheng@ubuntu_18_04:~$ sudo kubectl describe pod kubia-8jrw9
Events:
  Type     Reason                  Age                   From               Message
  ----     ------                  ----                  ----               -------
  Normal   Scheduled               19m                   default-scheduler  Successfully assigned kubia-8jrw9 to minikube
  Normal   SuccessfulMountVolume   19m                   kubelet, minikube  MountVolume.SetUp succeeded for volume "default-token-k7bkr"
  Warning  FailedCreatePodSandBox  4m51s (x31 over 19m)  kubelet, minikube  Failed create pod sandbox.
 
lucheng@ubuntu_18_04:~$ sudo kubectl get pods
 NAME          READY   STATUS    RESTARTS   AGE
kubia-qdqbz   1/1     Running   0          4m17s
```

查看pods状态一直是ContainerCreating，大概4min多钟后状态变成running

使用kubectl describe pod 查看event 

```
journalctl -u kubelet
```



删除pods

```
lucheng@ubuntu_18_04:~$ sudo kubectl get pods
NAME          READY   STATUS              RESTARTS   AGE
kubia-8jrw9   0/1     Running   			0        17h
lucheng@ubuntu_18_04:~$ sudo kubectl delete pods kubia-8jrw9
pod "kubia-8jrw9" deleted
lucheng@ubuntu_18_04:~$ sudo kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-rg2bs   1/1     Running   0          44s

//发现直接删除pod，又创建了一个新的pod，需要删除rc
lucheng@ubuntu_18_04:~$ sudo kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   1         1         0       17h
```



创建一个service object

**kubectl expose rc kubia --type=LoadBalancer --name kubia-http**

```
[root@master test]# kubectl expose rc kubia --type=LoadBalancer --name kubia-http
service/kubia-http exposed

[root@master test]# kubectl get services
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP      10.43.0.1       <none>        443/TCP          2d1h
kubia-http   LoadBalancer   10.43.137.236   <pending>     8080:30518/TCP   3s
```

然后可以通过服务访问，curl EXTERNAL-IP:8080 



问题：EXTERNAL-IP一直为pending状态？

https://blog.csdn.net/wxb880114/article/details/86535887

> Kubernetes does not offer an implementation of network load-balancers (Services of type LoadBalancer) for bare metal clusters. … If you’re not running on a supported IaaS platform (GCP, AWS, Azure…), LoadBalancers will remain in the “pending” state indefinitely when created



水平扩展

**kubectl scale rc kubia --replicas=3**

```
[root@master test]# kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         2       36m
[root@master test]# kubectl get pods
NAME          READY   STATUS              RESTARTS   AGE
kubia-6vbl6   0/1     ContainerCreating   0          3m9s
kubia-jp8gg   1/1     Running             0          3m9s
kubia-rg2bs   1/1     Running             0          27m
```



**Examining a YAML descriptor of an existing pod**

```
[root@master test]# kubectl get po kubia-6vbl6 -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2019-08-09T09:03:46Z"
  generateName: kubia-
  labels:
    run: kubia
  name: kubia-6vbl6
  namespace: default
  ownerReferences:

- apiVersion: v1
  blockOwnerDeletion: true
  controller: true
  kind: ReplicationController
  name: kubia
  uid: efa8c653-ba7f-11e9-ad1d-000c29bbd812
    resourceVersion: "45837"
    selfLink: /api/v1/namespaces/default/pods/kubia-6vbl6
    uid: 986e3797-ba84-11e9-ad1d-000c29bbd812
  spec:
    containers:
- image: chengludocker/kubia
  imagePullPolicy: Always
  name: kubia
  ports:
  - containerPort: 8080
    protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
  - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
    name: default-token-t6pd6
    readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
- effect: NoExecute
  key: node.kubernetes.io/not-ready
  operator: Exists
  tolerationSeconds: 300
- effect: NoExecute
  key: node.kubernetes.io/unreachable
  operator: Exists
  tolerationSeconds: 300
    volumes:
- name: default-token-t6pd6
  secret:
    defaultMode: 420
    secretName: default-token-t6pd6
  status:
    conditions:
- lastProbeTime: null
  lastTransitionTime: "2019-08-09T09:03:47Z"
  status: "True"
  type: Initialized
- lastProbeTime: null
  lastTransitionTime: "2019-08-09T09:07:18Z"
  status: "True"
  type: Ready
- lastProbeTime: null
  lastTransitionTime: "2019-08-09T09:07:18Z"
  status: "True"
  type: ContainersReady
- lastProbeTime: null
  lastTransitionTime: "2019-08-09T09:03:46Z"
  status: "True"
  type: PodScheduled
    containerStatuses:
- containerID: docker://710562d7921f2c0afd4efec3cb7913038e970dafd4591047fcd38dbbe25e5b54
  image: docker.io/chengludocker/kubia:latest
  imageID: docker-pullable://docker.io/chengludocker/kubia@sha256:562cda0f7697a25c1405f285003b3549cb2741d41cd8646946f9c693820bd7dc
  lastState: {}
  name: kubia
  ready: true
  restartCount: 0
  state:
    running:
      startedAt: "2019-08-09T09:07:17Z"
    hostIP: 192.168.159.129
    phase: Running
    podIP: 10.42.1.17
    qosClass: BestEffort
    startTime: "2019-08-09T09:03:47Z"
```
