## 20191105

[TOC]

### Services：enabling clients to discover and talk to pods

#### 三种IP介绍

* Node IP  			
* Pod IP 
* Cluster IP

Node节点的IP地址，Node IP 是 Kubernetes 集群中节点的物理⽹卡 IP 地址

Pod IP 是每个 Pod 的 IP 地址，Docker Engine 根据 docker0 ⽹桥的 IP 地址段进⾏分配的

Cluster IP 是⼀个虚拟的 IP，仅仅作用于Kubernetes Service这个对象，我们无法ping这个地址

#### 创建服务

第一种方式：使用kubectl expose

第二种方式：编写YAML文件，使用kubectl create提交创建

一个简单的示例

```yaml
apiVersion: v1
kind: Service			  #类型：服务
metadata:
  name: kubia
spec:
  ports:
  - name: http
    port: 80              #服务监听的端口 accept connections on port 80
    targetPort: 8080	  #route each connection to port 8080, 也可以是pod中定义的name
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
  sessionAffinity: None   #默认为None，可以设置为ClientIP，下面有具体介绍
  type：
```

使用kubectl get svc 查看新创建的服务

```shell
sudo kubectl get svc
NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubia 10.108.235.71 <none> 80/TCP 6m

sudo kubectl get pods
NAME                          READY     STATUS    RESTARTS   AGE
kubia-65mtj                   1/1       Running   0          27m
kubia-lnc7f                   1/1       Running   0          27m
kubia-vmgln                   1/1       Running   0          27m
```

可以看到服务的ip地址为10.108.235.71，这个是cluster-ip，只能在cluster内访问。

使用kubectl exec podname  访问cluster-ip

```
sudo kubectl exec kubia-65mtj  -- curl -s http://10.108.235.71
```

多次执行可以发现，service转发给后端pod是随机的，哪怕连接来自同一个请求方。如果希望来自同一个client IP的请求，转发到同一个后端pod，可以设置sessionAffinity:属性为ClientIP



#### 发现服务

**DISCOVERING SERVICES THROUGH ENVIRONMENT VARIABLES**

Environment variables are one way of looking up the IP and port of a service

```shell
sudo kubectl exec kubia-65mtj  env
KUBIA_SERVICE_HOST
KUBIA_SERVICE_PORT
```

**注意需要先创建service，后启动的pods才会有对应的环境变量**



**DISCOVERING SERVICES THROUGH DNS**

kube-system命名空间下面的kube-dns这个pod运行着一个DNS服务器，它知道所有运行的服务。

k8s通过修改每个容器的/etc/resolv.conf文件来使用dns服务器。

fully qualified domain name（FQDN）

比如backend-database[.default.svc.cluster.local]

backend-database对应服务名；default是服务对应的namespace；svc.cluster.local是域名后缀

进入到pod内验证

```shell
sudo kubectl exec kubia-8knvw -it bash
root@kubia-8knvw:/# curl http://kubia
```

**注意，clusterip是ping不通的**，因为cluster IP是一个虚拟IP

但是如何对外暴露服务呢？

#### 暴露外部服务

service endpoints介绍

服务不是直接跟pods关联的，一个叫Endpoints的资源存在于之间。

```shelll
lucheng@test:~$ sudo kubectl describe svc kubia
Name:              kubia
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=kubia
Type:              ClusterIP
IP:                10.108.235.71
Port:              <unset>  80/TCP
TargetPort:        8080/TCP
Endpoints:         172.17.0.11:8080,172.17.0.12:8080,172.17.0.13:8080
Session Affinity:  None
Events:            <none>
```

An Endpoints resource is a list of IP addresses，和service有着相同的名字



You have a few ways to make a service accessible externally

* Setting the service type to NodePort
* Setting the service type to LoadBalancer
* Creating an Ingress resource

a NodePort service can be accessed not only through the service’s internal cluster IP, but also
through any node’s IP and the reserved node port

如果service的type设置为NodePort，kubernetes将从给定的配置范围内（30000-32767）分配端口。所有节点上面都可以访问



a **LoadBalancer** service is an extension of a Node-Port service

if you only point your clients to the first node, when that node fails, your clients can’t access the service anymore. That’s why it makes sense to put a load balancer in front of the nodes to make sure you’re spreading requests across all healthy nodes and never sending them to a node that’s offline at that moment.



why **ingresses** are needed？

Each loadBalancer service requires its own load balancer with its own public IP address,

Ingress只需要一个来接收一群的services。

Nodeport需要管理大量的Port

#### Ingress

Ingress 其实就是从 kuberenets 集群外部访问集群的⼀个⼊⼝。根据host和path将外部的请求转发到集群内不同的Service 上，其实就相当于 nginx、haproxy 等负载均衡代理服务器。直接使用nginx不就行了？但是每次有新服务加入的时候，就需要修改nginx配置。

通过 Ingress，以实现使用 nginx 等开源的反向代理负载均衡器实现对外暴露服务。



使用 Ingress时一般会有三个组件：

* 反向代理负载均衡器
* Ingress Controller
* Ingress

反向代理负载均衡器就是nginx，apach之类的

Ingress Controller可以理解为监视器，通过不断的与kube-apiserver打交道，实时的感知后端的service和pod变化，更新反向代理负载均衡器。可以使用的Ingress controller有很多，比如traefik，nginx-controller

#### Traefik

Traefik 是⼀款开源的反向代理与负载均衡⼯具，性能较 nginx-controller 差，但是配置使⽤要简单许多。

**使用Traefik**

部署Traefik的Pod

创建Ingress对象

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-web-ui
  namespace: kube-system
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: traefik.haimaxy.com
    http:
      paths:
      - backend:
        serviceName: traefik-ingress-service
        servicePort: 8080
```

注意下rules区域，我们这⾥是要为 traefik 的 dashboard 建⽴⼀个 ingress 对象，所以这⾥的 serviceName 对应的是上⾯我们创建的 traefik-ingress-service，对应的端口是8080端口。

创建完成后，测试

本地/etc/hosts 添加traefik.haimaxy.com和node ip（the IP of the Ingress controller）的映射关系。

kubeclt get ingress 可以获取到ADDRESS。



annotations:

kubernetes.io/ingress.class: nginx

kubernetes.io/ingress.class: traefik

这个是必须的么？

是的。
