## 20191128

[TOC]

### Celery简介

Celery is a task queue.

Celery 通过消息机制进行通信。启动一个任务，客户端向消息队列发送一条消息。然后中间人（Broker）将消息传递给一个职程（Worker），最后由职程（Worker）进行执行中间人（Broker）分配的任务。

Celery 可以有多个职程（Worker）和中间人（Broker），用来提高Celery的高可用性以及横向扩展能力。

Celery 需要消息中间件RabbitMQ 或 Redis来进行发送和接收消息。 

```
# Django的设置文件settings.py，事实上就是在配置Django的django.conf.settings对象
from django.conf import settings
```

### 安装Celery

```
pip install celery
pip install redis
```

### 应用

```
# 启动一个redis
docker run -d -p 6379:6379 redis
```

新建一个app文件。创建第一个 Celery 实例程序；创建了一个名称为 add 的任务，返回的俩个数字的和。

```
from celery import Celery

app = Celery('tasks', broker='redis://localhost:6379/0')

@app.task
def add(x, y):
    return x + y
```

```
celery -A tasks worker --loglevel=info
```

**那么怎么调用我们创建的实例任务呢**

使用delay() ，是一种apply_async() 的快捷方

```
from tasks import add
result = add.delay(4, 4)
<AsyncResult: 10394aa1-f352-4ac2-b91a-5cd7c0520f0b>
result.ready()
报错AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'
```

通过控制台输出的日志进行查看,可以看到该任务已经有worker开始处理。

调用任务会返回一个 AsyncResult 的实例，用于检测任务的状态。默认这个功能是不开启的，需要配置celery的结果后端。

**保存结果**

如果需要跟踪任务的状态，Celery 需要在某处存储任务的状态信息，可以使用Redis作为Celery结果后端。

```
app = Celery('tasks', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')
```

### 配置

可以直接在程序中进行配置，也可以通过配置模块进行专门配置。例如，通过 task_serializer 选项可以指定序列化的方式

```
app.conf.task_serializer = 'json'

#更新多个配置
app.conf.update( task_serializer='json', result_serializer='json', enable_utc=True,)
```

针对大型项目，建议使用专用配置模块，将所有的配置项集中化配置。然后通过app.config_from_object()进行加载配置模块

新建celeryconfig.py文件

```
broker_url = 'redis://localhost:6379/0'
result_backend = 'redis://localhost:6379/0'

task_serializer = 'json'
result_serializer = 'json'
accept_content = ['json']
timezone = 'Europe/Oslo'
enable_utc = True
```

```
app.config_from_object('celeryconfig')
```

